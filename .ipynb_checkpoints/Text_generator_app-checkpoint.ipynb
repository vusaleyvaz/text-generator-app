{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "antique-messaging",
   "metadata": {},
   "source": [
    "# Text Generator App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-ranch",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.771Z"
    }
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "from math import pi\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import tf_top_k_top_p_filtering\n",
    "\n",
    "import panel as pn\n",
    "import panel.widgets as pnw\n",
    "from panel.template import DefaultTheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-schedule",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.779Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer and model for DistilGPT2\n",
    "gpt2_distil_tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "gpt2_distil_model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"distilgpt2\", pad_token_id=gpt2_distil_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-baseball",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.794Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer and model for GPT2(small)\n",
    "gpt2_small_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_small_model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", pad_token_id=gpt2_small_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-characteristic",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.803Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer and model for GPT2-Medium\n",
    "gpt2_medium_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "gpt2_medium_model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2-medium\", pad_token_id=gpt2_medium_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-opening",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.832Z"
    }
   },
   "outputs": [],
   "source": [
    "#adapted from tf_top_k_top_p_filtering function to filter probabilities rathen than logits\n",
    "#source - https://huggingface.co/transformers/v2.9.1/_modules/transformers/modeling_tf_utils.html\n",
    "def pr_top_k_top_p_filtering(probabilities,\n",
    "                             top_k=0,\n",
    "                             top_p=1.0,\n",
    "                             filter_value=-float(\"Inf\"),\n",
    "                             min_tokens_to_keep=1):\n",
    "    \"\"\" Filter a distribution of probabilities using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            probabilities: probabilities distribution shape (batch size, vocabulary size)\n",
    "            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "            Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    probabilities_shape = shape_list(probabilities)\n",
    "\n",
    "    if top_k > 0:\n",
    "        top_k = min(max(top_k, min_tokens_to_keep),probabilities_shape[-1])  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = probabilities < tf.math.top_k(probabilities, k=top_k)[0][..., -1,None]\n",
    "        probabilities = set_tensor_by_indices_to_value(probabilities, indices_to_remove,filter_value)\n",
    "\n",
    "    if top_p < 1.0:\n",
    "        sorted_indices = tf.argsort(probabilities, direction=\"DESCENDING\")\n",
    "        sorted_probabilities = tf.gather(\n",
    "            probabilities, sorted_indices, axis=-1, batch_dims=1\n",
    "        )  # expects logits to be of dim (batch_size, vocab_size)\n",
    "\n",
    "        cumulative_probs = tf.math.cumsum(sorted_probabilities, axis=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "\n",
    "        if min_tokens_to_keep > 1:\n",
    "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
    "            sorted_indices_to_remove = tf.concat(\n",
    "                [\n",
    "                    tf.zeros_like(\n",
    "                        sorted_indices_to_remove[:, :min_tokens_to_keep]),\n",
    "                    sorted_indices_to_remove[:, min_tokens_to_keep:],\n",
    "                ],\n",
    "                -1,\n",
    "            )\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove = tf.roll(sorted_indices_to_remove,1,axis=-1)\n",
    "        sorted_indices_to_remove = tf.concat(\n",
    "            [\n",
    "                tf.zeros_like(sorted_indices_to_remove[:, :1]),\n",
    "                sorted_indices_to_remove[:, 1:]\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = scatter_values_on_batch_indices(sorted_indices_to_remove, sorted_indices)\n",
    "        probabilities = set_tensor_by_indices_to_value(probabilities, indices_to_remove,filter_value)\n",
    "    return probabilities\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "\n",
    "def set_tensor_by_indices_to_value(tensor, indices, value):\n",
    "    # create value_tensor since tensor value assignment is not possible in TF\n",
    "    value_tensor = tf.zeros_like(tensor) + value\n",
    "    return tf.where(indices, value_tensor, tensor)\n",
    "\n",
    "\n",
    "def scatter_values_on_batch_indices(values, batch_indices):\n",
    "    shape = shape_list(batch_indices)\n",
    "    # broadcast batch dim to shape\n",
    "    broad_casted_batch_dims = tf.reshape(\n",
    "        tf.broadcast_to(tf.expand_dims(tf.range(shape[0]), axis=-1), shape),\n",
    "        [1, -1])\n",
    "    # transform batch_indices to pair_indices\n",
    "    pair_indices = tf.transpose(\n",
    "        tf.concat(\n",
    "            [broad_casted_batch_dims,\n",
    "             tf.reshape(batch_indices, [1, -1])], 0))\n",
    "    # scatter values to pair indices\n",
    "    return tf.scatter_nd(pair_indices, tf.reshape(values, [-1]), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-kansas",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.841Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next_token_logits(sequence='Please input some text',\n",
    "                          model=gpt2_small_model,\n",
    "                          tokenizer=gpt2_small_tokenizer):\n",
    "    '''\n",
    "    Get the logits to derive the probabilities for each prediction.\n",
    "    - the last layer of the logits output provides the logits for the next token\n",
    "    '''\n",
    "    input_ids = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "    next_token_logits = model(input_ids).logits[:, -1, :]\n",
    "    return next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-traveler",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.853Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prediction(next_token_logits=[[]],\n",
    "                   decoding_type='Greedy Search',\n",
    "                   model=gpt2_small_model,\n",
    "                   tokenizer=gpt2_small_tokenizer,\n",
    "                   temperature=0.7,\n",
    "                   top_k=50,\n",
    "                   top_p=0.95):\n",
    "    \"\"\"\n",
    "    Get the prediction for the next word\n",
    "    Greedy search -  returns the word with highest probability\n",
    "    Sampling - sample based on filtering\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(60)\n",
    "    if decoding_type == \"Greedy Search\":\n",
    "        next_token = tf.math.argmax(next_token_logits,\n",
    "                                    axis=-1,\n",
    "                                    output_type=tf.int32)\n",
    "    elif decoding_type == \"Sampling\":\n",
    "        # apply a Temperature\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "        # filter to extract logits\n",
    "        filtered_next_token_logits = tf_top_k_top_p_filtering(\n",
    "            next_token_logits, top_k, top_p)\n",
    "        next_token = tf.random.categorical(filtered_next_token_logits,\n",
    "                                           dtype=tf.int32,\n",
    "                                           num_samples=1)\n",
    "    prediction = tokenizer.decode(next_token.numpy().tolist()[0])\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-ontario",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.868Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_next_token_probabilities(next_token_logits=[[]],\n",
    "                                    decoding_type='Greedy Search',\n",
    "                                    model=gpt2_small_model,\n",
    "                                    tokenizer=gpt2_small_tokenizer,\n",
    "                                    temperature=0.7,\n",
    "                                    top_k=50,\n",
    "                                    top_p=0.95):\n",
    "    \"\"\"\n",
    "    Get the probabilities for the next word options\n",
    "    Greedy search -  returns the word with highest probability\n",
    "    Sampling - sample based on filtering\n",
    "    \"\"\"\n",
    "    if decoding_type == \"Greedy Search\":\n",
    "        filtered_next_token_probabilities = tf.nn.softmax(next_token_logits)\n",
    "    elif decoding_type == \"Sampling\":\n",
    "        # apply a Temperature\n",
    "        if temperature != 1.0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "        # convert logits to probabilities\n",
    "        next_token_probabilities = tf.nn.softmax(next_token_logits)\n",
    "        # filter to extract probabilities\n",
    "        filtered_next_token_probabilities = pr_top_k_top_p_filtering(\n",
    "            next_token_probabilities, top_k, top_p)\n",
    "    return filtered_next_token_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-american",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.880Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_plot_data(filtered_next_token_probabilities, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the list of words and probabilities for plotting\n",
    "    \"\"\"\n",
    "    #convert -inf values to zero\n",
    "    probabilities = tf.nn.relu(filtered_next_token_probabilities)\n",
    "    #count non-zero values\n",
    "    k = tf.math.count_nonzero(probabilities).numpy()\n",
    "    #to limit the print barplot on screen to 100 words\n",
    "    k = min(100, k)\n",
    "    #extract top k probabilities\n",
    "    filtered_probabilities_data = tf.math.top_k(probabilities[0], k)\n",
    "    #convert probabilities to list\n",
    "    filtered_probabilities = filtered_probabilities_data.values.numpy()\n",
    "    probability_list = filtered_probabilities.tolist()\n",
    "    #get list of words\n",
    "    word_list = list()\n",
    "    for i in filtered_probabilities_data.indices.numpy():\n",
    "        word_list.append(tokenizer.decode([i]))\n",
    "    return word_list, probability_list\n",
    "\n",
    "\n",
    "def clean_plot_data(word_list, probability_list):\n",
    "    \"\"\"\n",
    "    Prepares the data for plotting\n",
    "    - Aggregates words that appear multiple times\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for w, p in zip(word_list, probability_list):\n",
    "        if w not in result:\n",
    "            result[w] = p\n",
    "        else:\n",
    "            result[w] += p\n",
    "\n",
    "    sorted_keys = sorted(result, key=result.get, reverse=True)\n",
    "    result = {k: result[k] for k in sorted_keys}\n",
    "    return list(result.keys()), list(result.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-volunteer",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.888Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_plot(word_list, probability_list):\n",
    "    \"\"\"\n",
    "    Get the plot data for Bokeh plot\n",
    "    \"\"\"    \n",
    "    source = ColumnDataSource(\n",
    "        data=dict(word_list=word_list, probability_list=probability_list))\n",
    "    plot = figure(x_range=source.data['word_list'],\n",
    "                  height=250,\n",
    "                  title=\"Probabilities\",\n",
    "                  #define toolbar location\n",
    "                  toolbar_location=\"right\",\n",
    "                  #add interactivity to the plot by different tools\n",
    "                  tools=\"hover,pan,wheel_zoom,box_zoom,zoom_in,zoom_out,reset\",\n",
    "                  #define displayed text while hovering through the plot\n",
    "                  tooltips=\"@word_list: @probability_list{0.0000}\")\n",
    "    plot.vbar(x='word_list',\n",
    "              top='probability_list',\n",
    "              width=0.8,\n",
    "              color='#d45781',\n",
    "              source=source)\n",
    "    plot.xaxis.major_label_orientation = pi / 2\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-louisville",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.894Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_perplexity(sequence, gpt2_model, gpt2_tokenizer):\n",
    "    \"\"\"\n",
    "    Get perplexity for the given sequence\n",
    "    \"\"\"    \n",
    "    input_ids = gpt2_tokenizer.encode(sequence, return_tensors=\"tf\")\n",
    "    loss = gpt2_model(input_ids=input_ids, labels=input_ids).loss\n",
    "    perplexity = tf.math.exp(tf.math.reduce_mean(loss)).numpy()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-immigration",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.899Z"
    }
   },
   "outputs": [],
   "source": [
    "pn.extension(sizing_mode='stretch_width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-candy",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.904Z"
    }
   },
   "outputs": [],
   "source": [
    "#widget to choose the model\n",
    "model_pn = pn.widgets.Select(options=['DistilGPT2', 'GPT2', 'GPT2-Medium'])\n",
    "\n",
    "#widget to choose the decoding method\n",
    "decoding_pn = pn.widgets.RadioBoxGroup(name='RadioBoxGroup',\n",
    "                                       options=['Greedy Search', 'Sampling'],\n",
    "                                       inline=True)\n",
    "\n",
    "# widgets for sampling methods to control the model output for the predictions\n",
    "temperature_pn = pnw.FloatSlider(name='Temperature',\n",
    "                                 value=1.0,\n",
    "                                 start=0.01,\n",
    "                                 end=3.0,\n",
    "                                 step=0.01,\n",
    "                                 bar_color='#d45781')\n",
    "top_k_pn = pnw.IntSlider(name='Top K',\n",
    "                         value=0,\n",
    "                         start=0,\n",
    "                         end=100,\n",
    "                         bar_color='#d45781')\n",
    "top_p_pn = pnw.FloatSlider(name='Top p',\n",
    "                           value=1.0,\n",
    "                           start=0.0,\n",
    "                           end=1.0,\n",
    "                           step=0.01,\n",
    "                           bar_color='#d45781')\n",
    "next_words_pn = pnw.IntSlider(name='Number of next predicted words',\n",
    "                              value=1,\n",
    "                              start=1,\n",
    "                              end=30,\n",
    "                              bar_color='#d45781')\n",
    "\n",
    "#text input widget for initial prompt\n",
    "text_input = pn.widgets.TextInput(placeholder='Enter a string here...')\n",
    "\n",
    "#pane for generated text output\n",
    "generated_text = pn.pane.HTML(object=text_input.value,\n",
    "                              background='#f0f0f0',\n",
    "                              min_height=200,\n",
    "                              sizing_mode=\"stretch_width\")\n",
    "\n",
    "text_input.link(generated_text, value='object')\n",
    "\n",
    "#button widge for string the text generation\n",
    "button = pn.widgets.Button(name=\"Generate\", button_type='primary')\n",
    "\n",
    "# Bokeh bar plot initialization and Bokeh plot pane\n",
    "word_list = list()\n",
    "probability_list = list()\n",
    "plot = get_plot(word_list, probability_list)\n",
    "bokeh_plot = pn.pane.Bokeh(plot, sizing_mode=\"stretch_width\")\n",
    "\n",
    "#pane for perplexity calculation output display\n",
    "perplexity_pn = pn.pane.HTML(object=\"Perplexity: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-induction",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.909Z"
    }
   },
   "outputs": [],
   "source": [
    "#Panels for creating the parts of the application\n",
    "model_widget = pn.Column(\"#Model\", model_pn, )\n",
    "\n",
    "decoding_widget = pn.Column(\"##\",\"#Decoding Method\",decoding_pn)\n",
    "\n",
    "#create empty panel, and fill with widgets if Sampling is chosen\n",
    "parameter_widgets = pn.Column()\n",
    "\n",
    "#Header for Sampling parameters, created separately to be able to hide/show depending on Sampling is chosen or not\n",
    "header_parameters = pn.pane.Markdown(\"\"\"\n",
    "##\n",
    "\n",
    "#Parameters\n",
    "\"\"\")\n",
    "\n",
    "#Panel for setting the repetition of the text generation process\n",
    "repeat_widget = pn.Column(\"##\",\"#Repeat Prediction\",next_words_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-abraham",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.912Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to hide/show the parameters depending on the chosen decoding method\n",
    "def hide_parameters(event):\n",
    "    if event.new == 'Sampling':\n",
    "        parameter_widgets.append(header_parameters)\n",
    "        parameter_widgets.append(temperature_pn)\n",
    "        parameter_widgets.append(top_k_pn)\n",
    "        parameter_widgets.append(top_p_pn)\n",
    "    else:\n",
    "        parameter_widgets.remove(header_parameters)\n",
    "        parameter_widgets.remove(temperature_pn)\n",
    "        parameter_widgets.remove(top_k_pn)\n",
    "        parameter_widgets.remove(top_p_pn)  \n",
    "        \n",
    "#tying hide_parameters function to the decoding_pn\n",
    "decoding_pn.param.watch(hide_parameters, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-armenia",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.919Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to update data by button click\n",
    "def click_cb(event):\n",
    "    for i in range(next_words_pn.value):\n",
    "        bokeh_plot.loading = True\n",
    "        if model_pn.value == 'DistilGPT2':\n",
    "            next_token_logits = get_next_token_logits(generated_text.object,\n",
    "                                                      gpt2_distil_model,\n",
    "                                                      gpt2_distil_tokenizer)\n",
    "            pred = get_prediction(next_token_logits, decoding_pn.value,\n",
    "                                  gpt2_distil_model, gpt2_distil_tokenizer,\n",
    "                                  temperature_pn.value, top_k_pn.value,\n",
    "                                  top_p_pn.value)\n",
    "            filtered_next_token_probabilities = filter_next_token_probabilities(\n",
    "                next_token_logits, decoding_pn.value, gpt2_distil_model,\n",
    "                gpt2_distil_tokenizer, temperature_pn.value, top_k_pn.value,\n",
    "                top_p_pn.value)\n",
    "            word_list, probability_list = get_plot_data(\n",
    "                filtered_next_token_probabilities, gpt2_distil_tokenizer)\n",
    "            generated_text.object += pred\n",
    "            perplexity = get_perplexity(generated_text.object,\n",
    "                                        gpt2_distil_model,\n",
    "                                        gpt2_distil_tokenizer)\n",
    "        elif model_pn.value == 'GPT2':\n",
    "            next_token_logits = get_next_token_logits(generated_text.object,\n",
    "                                                      gpt2_small_model,\n",
    "                                                      gpt2_small_tokenizer)\n",
    "            pred = get_prediction(next_token_logits, decoding_pn.value,\n",
    "                                  gpt2_small_model, gpt2_small_tokenizer,\n",
    "                                  temperature_pn.value, top_k_pn.value,\n",
    "                                  top_p_pn.value)\n",
    "            filtered_next_token_probabilities = filter_next_token_probabilities(\n",
    "                next_token_logits, decoding_pn.value, gpt2_small_model,\n",
    "                gpt2_small_tokenizer, temperature_pn.value, top_k_pn.value,\n",
    "                top_p_pn.value)\n",
    "            word_list, probability_list = get_plot_data(\n",
    "                filtered_next_token_probabilities, gpt2_small_tokenizer)\n",
    "            generated_text.object += pred\n",
    "            perplexity = get_perplexity(generated_text.object,\n",
    "                                        gpt2_small_model, gpt2_small_tokenizer)\n",
    "        elif model_pn.value == 'GPT2-Medium':\n",
    "            next_token_logits = get_next_token_logits(generated_text.object,\n",
    "                                                      gpt2_medium_model,\n",
    "                                                      gpt2_medium_tokenizer)\n",
    "            pred = get_prediction(next_token_logits, decoding_pn.value,\n",
    "                                  gpt2_medium_model, gpt2_medium_tokenizer,\n",
    "                                  temperature_pn.value, top_k_pn.value,\n",
    "                                  top_p_pn.value)\n",
    "            filtered_next_token_probabilities = filter_next_token_probabilities(\n",
    "                next_token_logits, decoding_pn.value, gpt2_medium_model,\n",
    "                gpt2_medium_tokenizer, temperature_pn.value, top_k_pn.value,\n",
    "                top_p_pn.value)\n",
    "            word_list, probability_list = get_plot_data(\n",
    "                filtered_next_token_probabilities, gpt2_medium_tokenizer)\n",
    "            generated_text.object += pred\n",
    "            perplexity = get_perplexity(generated_text.object,\n",
    "                                        gpt2_medium_model,\n",
    "                                        gpt2_medium_tokenizer)\n",
    "\n",
    "        perplexity_pn.object = f\"Perplexity:\\n {perplexity}\"\n",
    "\n",
    "        word_list, probability_list = clean_plot_data(word_list,\n",
    "                                                      probability_list)\n",
    "\n",
    "        bokeh_plot.object = get_plot(word_list, probability_list)\n",
    "        bokeh_plot.loading = False\n",
    "\n",
    "\n",
    "# update data by button click\n",
    "button.on_click(click_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-panic",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.930Z"
    }
   },
   "outputs": [],
   "source": [
    "# callback function in case the text input changes\n",
    "def text_change_cb(event):\n",
    "    generated_text.object = event.new\n",
    "\n",
    "# tying the callback function to the text_input widget\n",
    "text_input.param.watch(text_change_cb, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-webcam",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.935Z"
    }
   },
   "outputs": [],
   "source": [
    "#Build applcation using FastListTemplate\n",
    "application = pn.template.FastListTemplate(\n",
    "    title=\"Text Generator\",\n",
    "    sidebar=[model_widget, decoding_widget, parameter_widgets, repeat_widget],\n",
    "    main=[text_input, button, generated_text, bokeh_plot, perplexity_pn],\n",
    "    theme=DefaultTheme,\n",
    "    theme_toggle=False,\n",
    "    accent_base_color='#d45781',\n",
    "    header_background='#d45781')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-representative",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-12-23T17:00:22.942Z"
    }
   },
   "outputs": [],
   "source": [
    "application.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
